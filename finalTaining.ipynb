{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finalTaining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRv-urycpDKO",
        "colab_type": "text"
      },
      "source": [
        "# This Notebook is to train the model using github repositry on google colab:\n",
        "**The steps of doing so are:**\n",
        "\n",
        "\n",
        "1.   Mount the drive \n",
        "2.   Creating a path to store the github repo files \n",
        "3.   Creating a GIT token to access the files \n",
        "4.   %cd to the path then clonning the repo \n",
        "5.   Importing the files (the method is shown below in the cell)\n",
        "6.   Creating our train function then running it \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u0UvS8jqQ0Q",
        "colab_type": "text"
      },
      "source": [
        "# Mounting the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksGwDuXA3vr4",
        "colab_type": "code",
        "outputId": "effded2e-fc68-433c-8884-6887e718bf17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT) "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP80vddWqXem",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Path and GIT token and making the directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DmkUV7T4DXP",
        "colab_type": "code",
        "outputId": "d1f7fef4-b9f5-4dc5-8c9b-3df4a63c2b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from os.path import join  \n",
        "\n",
        "# path to your project on Google Drive\n",
        "MY_GOOGLE_DRIVE_PATH = 'My Drive/RL_Tag_GameFinal' \n",
        "# replace with your Github username \n",
        "GIT_USERNAME = \"username\" \n",
        "# definitely replace with your\n",
        "GIT_TOKEN = \"{xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx}\"  \n",
        "# Replace with your github repository in this case we want \n",
        "# to clone deep-learning-v2-pytorch repository\n",
        "GIT_REPOSITORY = \"RL_TAG\" \n",
        "\n",
        "PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
        "\n",
        "# It's good to print out the value if you are not sure \n",
        "print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
        "\n",
        "# In case we haven't created the folder already; we will create a folder in the project path \n",
        "!mkdir \"{PROJECT_PATH}\"    \n",
        "\n",
        "#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n",
        "GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "print(\"GIT_PATH: \", GIT_PATH)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PROJECT_PATH:  /content/drive/My Drive/RL_Tag_GameFinal\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/RL_Tag_GameFinal’: File exists\n",
            "GIT_PATH:  https://{74b3c3dfea907d13d3490e356b8f1246ce3a7b86}@github.com/marwanihab/RL_TAG.git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUtVa5_5qg0r",
        "colab_type": "text"
      },
      "source": [
        "# %cd to the directory and git pull "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSMoJuUf4OKw",
        "colab_type": "code",
        "outputId": "8d55e20e-db36-42a0-aa1d-e05f44b59223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd {PROJECT_PATH}\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/RL_Tag_GameFinal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL8PxM5R4TYD",
        "colab_type": "code",
        "outputId": "6edbca7a-7864-426e-f075-224d2e4b9a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone {GIT_PATH}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'RL_TAG' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTgHz4Sd4W1Y",
        "colab_type": "code",
        "outputId": "9546750a-1b99-4ea5-fd88-cd942111e710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd RL_TAG/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/RL_Tag_GameFinal/RL_TAG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTT4yX5X4YOt",
        "colab_type": "code",
        "outputId": "c65d0624-b03a-4649-e736-5ddf331d6994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git config --global user.email \"marwan.ihab95@gmail.com\" \n",
        "!git stash"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved working directory and index state WIP on master: 6dccc7e remove print statements\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0juQQ1tnk-t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "882ec96a-eef6-447a-d00a-6a3286f5ac28"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updating 6dccc7e..31992d3\n",
            "Fast-forward\n",
            " .DS_Store                                          | Bin \u001b[31m12292\u001b[m -> \u001b[32m10244\u001b[m bytes\n",
            " .idea/.gitignore                                   |   3 \u001b[32m++\u001b[m\n",
            " .idea/SecondTrial.iml                              |  12 \u001b[32m++++++++\u001b[m\n",
            " .idea/inspectionProfiles/profiles_settings.xml     |   6 \u001b[32m++++\u001b[m\n",
            " .idea/misc.xml                                     |   7 \u001b[32m+++++\u001b[m\n",
            " .idea/modules.xml                                  |   8 \u001b[32m+++++\u001b[m\n",
            " .idea/vcs.xml                                      |   6 \u001b[32m++++\u001b[m\n",
            " checkpoints-20200401T205227Z-001.zip               | Bin \u001b[31m0\u001b[m -> \u001b[32m586076\u001b[m bytes\n",
            " checkpoints-21.zip                                 | Bin \u001b[31m0\u001b[m -> \u001b[32m586424\u001b[m bytes\n",
            " checkpoints/.DS_Store                              | Bin \u001b[31m6148\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_0.pth                 | Bin \u001b[31m79296\u001b[m -> \u001b[32m77184\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_1.pth                 | Bin \u001b[31m79296\u001b[m -> \u001b[32m76128\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_2.pth                 | Bin \u001b[31m79296\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_3.pth                 | Bin \u001b[31m78240\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_target_0.pth          | Bin \u001b[31m79296\u001b[m -> \u001b[32m77184\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_target_1.pth          | Bin \u001b[31m79296\u001b[m -> \u001b[32m76128\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_target_2.pth          | Bin \u001b[31m79296\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_target_3.pth          | Bin \u001b[31m78240\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_0.pth                | Bin \u001b[31m112058\u001b[m -> \u001b[32m77738\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_1.pth                | Bin \u001b[31m112058\u001b[m -> \u001b[32m85658\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_2.pth                | Bin \u001b[31m112058\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_3.pth                | Bin \u001b[31m78794\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_target_0.pth         | Bin \u001b[31m112058\u001b[m -> \u001b[32m77738\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_target_1.pth         | Bin \u001b[31m112058\u001b[m -> \u001b[32m85658\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_target_2.pth         | Bin \u001b[31m112058\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_target_3.pth         | Bin \u001b[31m78794\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " .../__pycache__/simple_tag.cpython-37.pyc          | Bin \u001b[31m5395\u001b[m -> \u001b[32m4946\u001b[m bytes\n",
            " multiagent/scenarios/simple_tag.py                 |  34 \u001b[32m++++++++++\u001b[m\u001b[31m-----------\u001b[m\n",
            " 28 files changed, 59 insertions(+), 17 deletions(-)\n",
            " create mode 100644 .idea/.gitignore\n",
            " create mode 100644 .idea/SecondTrial.iml\n",
            " create mode 100644 .idea/inspectionProfiles/profiles_settings.xml\n",
            " create mode 100644 .idea/misc.xml\n",
            " create mode 100644 .idea/modules.xml\n",
            " create mode 100644 .idea/vcs.xml\n",
            " create mode 100644 checkpoints-20200401T205227Z-001.zip\n",
            " create mode 100644 checkpoints-21.zip\n",
            " delete mode 100644 checkpoints/.DS_Store\n",
            " delete mode 100644 checkpoints/checkpoint_actor_2.pth\n",
            " delete mode 100644 checkpoints/checkpoint_actor_3.pth\n",
            " delete mode 100644 checkpoints/checkpoint_actor_target_2.pth\n",
            " delete mode 100644 checkpoints/checkpoint_actor_target_3.pth\n",
            " delete mode 100644 checkpoints/checkpoint_critic_2.pth\n",
            " delete mode 100644 checkpoints/checkpoint_critic_3.pth\n",
            " delete mode 100644 checkpoints/checkpoint_critic_target_2.pth\n",
            " delete mode 100644 checkpoints/checkpoint_critic_target_3.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC7e50I54a0G",
        "colab_type": "code",
        "outputId": "aaec019d-4f95-4be4-f3cc-e36e2d34a4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "!pip uninstall gym\n",
        "!pip install gym==0.10.5"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling gym-0.17.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/gym-0.17.1.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/gym/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled gym-0.17.1\n",
            "Collecting gym==0.10.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/50/ed4a03d2be47ffd043be2ee514f329ce45d98a30fe2d1b9c61dea5a9d861/gym-0.10.5.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (1.18.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (1.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym==0.10.5) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.5-cp36-none-any.whl size=1581309 sha256=640ac5ab74ae8e0b1e7d607f9e80a84ca6b37db7ab0b6641645b5e9b93475712\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/14/71/f4ab006b1e6ff75c2b54985c2f98d0644fffe9c1dddc670925\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "Successfully installed gym-0.10.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSzc6peaqs3a",
        "colab_type": "text"
      },
      "source": [
        "# Most important step creating a modules for the necessary \".py\" file that we need to import to run the train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RklAbpwF4h88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imp \n",
        "Agent = imp.new_module('agent')\n",
        "exec(open(\"./agent.py\").read(), Agent.__dict__)\n",
        "\n",
        "actor_critic_model = imp.new_module('actor_critic_model')\n",
        "exec(open(\"./actor_critic_model.py\").read(), actor_critic_model.__dict__)\n",
        "\n",
        "ornsteinUhlenbeck = imp.new_module('ornsteinUhlenbeck')\n",
        "exec(open(\"./ornsteinUhlenbeck.py\").read(), ornsteinUhlenbeck.__dict__)\n",
        "\n",
        "replay_buffer = imp.new_module('replay_buffer')\n",
        "exec(open(\"./replay_buffer.py\").read(), replay_buffer.__dict__)\n",
        "\n",
        "core = imp.new_module('core.py')\n",
        "exec(open(\"./multiagent/core.py\").read(), core.__dict__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WLIu-FW4nU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "environment = imp.new_module('environment.py')\n",
        "exec(open(\"./multiagent/environment.py\").read(), environment.__dict__)\n",
        "\n",
        "multi_discrete = imp.new_module('multi_discrete.py')\n",
        "exec(open(\"./multiagent/multi_discrete.py\").read(), multi_discrete.__dict__)\n",
        "\n",
        "# policy = imp.new_module('policy.py')\n",
        "# exec(open(\"./multiagent/policy.py\").read(), policy.__dict__)\n",
        "\n",
        "# rendering = imp.new_module('rendering.py')\n",
        "# exec(open(\"./multiagent/rendering.py\").read(), rendering.__dict__)\n",
        "\n",
        "scenario = imp.new_module('scenario.py')\n",
        "exec(open(\"./multiagent/scenario.py\").read(), scenario.__dict__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqzyHn3s4qjH",
        "colab_type": "code",
        "outputId": "93e7ca45-4fef-4be8-9ea0-1f08d07e1e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "!pip show torchvision"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: torchvision\n",
            "Version: 0.5.0\n",
            "Summary: image and video datasets and models for torch deep learning\n",
            "Home-page: https://github.com/pytorch/vision\n",
            "Author: PyTorch Core Team\n",
            "Author-email: soumith@pytorch.org\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: pillow, torch, numpy, six\n",
            "Required-by: fastai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hsTxEwo40GC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "from agent import Agent\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import pickle\n",
        "from gym.spaces import Box\n",
        "from actor_critic_model import Actor, Critic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RdQIFRP47cF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_env(scenario_name, arglist):\n",
        "    from multiagent.environment import MultiAgentEnv\n",
        "    import multiagent.scenarios as scenarios\n",
        "\n",
        "    # load scenario from script\n",
        "    scenario = scenarios.load(scenario_name + \".py\").Scenario()\n",
        "    # create world\n",
        "    world = scenario.make_world()\n",
        "    # create multiagent environment\n",
        "    if arglist.benchmark:\n",
        "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation, scenario.benchmark_data)\n",
        "    else:\n",
        "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)\n",
        "    return env\n",
        "\n",
        "\n",
        "def _algo_mode_from_agents(env):\n",
        "    algo_mode = []\n",
        "\n",
        "    for agent in env.agents:\n",
        "        if agent.adversary:  # adversary\n",
        "            algo_mode.append('DDPG')  # MADDPG\n",
        "        else:\n",
        "            algo_mode.append('MADDPG')\n",
        "    return algo_mode\n",
        "\n",
        "\n",
        "def create_agents(env, arglist):\n",
        "    agents = []\n",
        "    algo_mode = _algo_mode_from_agents(env=env)\n",
        "\n",
        "    obs_shapes = [env.observation_space[i].shape for i in range(env.n)]\n",
        "    actions_shape_n = [env.action_space[i].n for i in range(env.n)]\n",
        "    actions_n = 0\n",
        "    obs_shape_n = 0\n",
        "\n",
        "    for actions in actions_shape_n:\n",
        "        actions_n += actions\n",
        "    for obs_shape in obs_shapes:\n",
        "        obs_shape_n += obs_shape[0]\n",
        "\n",
        "    for i, action_space, observation_space, algo in zip(range(len(env.action_space)), env.action_space,\n",
        "                                                        env.observation_space, algo_mode):\n",
        "\n",
        "        if isinstance(action_space, Box):\n",
        "            discrete_action = False\n",
        "        else:\n",
        "            discrete_action = True\n",
        "\n",
        "        if algo == 'MADDPG':\n",
        "            print('MADDPG load.')\n",
        "            critic = Critic(obs_shape_n, actions_n).to(device)\n",
        "            actor = Actor(observation_space.shape[0], action_space.n).to(device)\n",
        "            target_critic = Critic(obs_shape_n, actions_n, arglist.tau).to(device)\n",
        "            target_actor = Actor(observation_space.shape[0], action_space.n, arglist.tau).to(device)\n",
        "        else:\n",
        "            print('DDPG load.')\n",
        "            critic = Critic(observation_space.shape[0], action_space.n).to(device)\n",
        "            actor = Actor(observation_space.shape[0], action_space.n).to(device)\n",
        "            target_critic = Critic(observation_space.shape[0], action_space.n, arglist.tau).to(device)\n",
        "            target_actor = Actor(observation_space.shape[0], action_space.n, arglist.tau).to(device)\n",
        "        actor.eval()\n",
        "        critic.eval()\n",
        "        target_actor.eval()\n",
        "        target_critic.eval()\n",
        "        agents.append(\n",
        "            Agent(i, actor, critic, target_actor, target_critic, arglist.eval, discrete_action, arglist, algo))\n",
        "    return agents\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMZE3AoK5Adg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(arglist):\n",
        "    env = make_env(scenario_name=\"simple_tag\", arglist=arglist)\n",
        "    # ACTORS = 1\n",
        "    # env = EnvWrapper(arglist.scenario, ACTORS, arglist.saved_episode)\n",
        "    agents = create_agents(env, arglist)\n",
        "    max_episode_len = 25\n",
        "\n",
        "    if arglist.display:\n",
        "        for i in range(len(agents)):\n",
        "            actor = agents[i].actor\n",
        "            actor_ckpt = torch.load('./checkpoints/checkpoint_actor_{}.pth'.format(i), map_location='cpu')\n",
        "            actor.load_state_dict(actor_ckpt)\n",
        "            actor_target = agents[i].actor_target\n",
        "            actor_target_ckpt = torch.load('./checkpoints/checkpoint_actor_target_{}.pth'.format(i), map_location='cpu')\n",
        "            actor_target.load_state_dict(actor_target_ckpt)\n",
        "            critic = agents[i].critic\n",
        "            critic_ckpt = torch.load('./checkpoints/checkpoint_critic_{}.pth'.format(i), map_location='cpu')\n",
        "            critic.load_state_dict(critic_ckpt)\n",
        "            critic_target = agents[i].critic_target\n",
        "            critic_target_ckpt = torch.load('./checkpoints/checkpoint_critic_target_{}.pth'.format(i),\n",
        "                                            map_location='cpu')\n",
        "            critic_target.load_state_dict(critic_target_ckpt)\n",
        "\n",
        "    final_ep_rewards = []\n",
        "    final_ep_ag_rewards = []\n",
        "    episode_rewards = [0.0]\n",
        "    agent_rewards = [[0.0] for _ in range(env.n)]\n",
        "    agent_info = [[[]]]\n",
        "    obs_n = env.reset()\n",
        "    episode_step = 0\n",
        "    train_step = 0\n",
        "    t_start = time.time()\n",
        "\n",
        "    print('Starting iterations...')\n",
        "    while True:\n",
        "        # get action\n",
        "        for agent in agents:\n",
        "            agent.reset()\n",
        "\n",
        "        # action_n = []\n",
        "        # for obs in obs_n:\n",
        "        #     actions = []\n",
        "        #     for i, agent in enumerate(agents):\n",
        "        #         action = agent.act(obs[i], add_noise=False)\n",
        "        #         actions.append(action)\n",
        "        #     action_n.append(actions)\n",
        "\n",
        "        action_n = [agent.act(obs, add_noise=False) for agent, obs in zip(agents, obs_n)]\n",
        "\n",
        "        # environment step\n",
        "        new_obs_n, rew_n, done_n, info_n = env.step(action_n)\n",
        "        episode_step += 1\n",
        "        done = all(done_n)\n",
        "        terminal = (episode_step >= max_episode_len)\n",
        "        # collect experience\n",
        "        for i, agent in enumerate(agents):\n",
        "            agent.experience(obs_n[i], action_n[i], rew_n[i], new_obs_n[i], done_n[i])\n",
        "        obs_n = new_obs_n\n",
        "        # print(rew_n)\n",
        "        for i, rew in enumerate(rew_n):\n",
        "            episode_rewards[-1] += rew\n",
        "            agent_rewards[i][-1] += rew\n",
        "\n",
        "        if done or terminal:\n",
        "            obs_n = env.reset()\n",
        "            episode_step = 0\n",
        "            episode_rewards.append(0)\n",
        "            # print(episode_rewards)\n",
        "            for a in agent_rewards:\n",
        "                a.append(0)\n",
        "            agent_info.append([[]])\n",
        "\n",
        "        # increment global step counter\n",
        "        train_step += 1\n",
        "        # for benchmarking learned policies\n",
        "        if arglist.benchmark:\n",
        "            for i, info in enumerate(info_n):\n",
        "                agent_info[-1][i].append(info_n['n'])\n",
        "            if train_step > arglist.benchmark_iters and (done or terminal) and (len(episode_rewards) % 1000 == 0):\n",
        "                file_name = arglist.benchmark_dir + arglist.exp_name + '.pkl'\n",
        "                print('Finished benchmarking, now saving...')\n",
        "                with open(file_name, 'wb') as fp:\n",
        "                    pickle.dump(agent_info[:-1], fp)\n",
        "                # break\n",
        "            # continue\n",
        "\n",
        "        # for displaying learned policies\n",
        "        if arglist.display:\n",
        "            time.sleep(0.1)\n",
        "            env.render()\n",
        "            continue\n",
        "\n",
        "        # update all trainers, if not in display or benchmark mode\n",
        "        loss = None\n",
        "        for agent in agents:\n",
        "            agent.preupdate()\n",
        "        for agent in agents:\n",
        "            loss = agent.step(agents, train_step, terminal)\n",
        "\n",
        "        # save model, display training output\n",
        "        if terminal and (len(episode_rewards) % 1000 == 0):  # 25 and 1000\n",
        "\n",
        "            print(\"steps: {}, episodes: {}, mean episode reward: {}, time: {}\".format(\n",
        "                train_step, len(episode_rewards), np.mean(episode_rewards[-1000:]), round(time.time() - t_start, 3)))\n",
        "            # plotter.plot('Episode Rewards', 'Rewards', 'Training', len(episode_rewards),\n",
        "            #              np.mean(episode_rewards[-1000:]))\n",
        "            i = 0\n",
        "            for agt in agents:\n",
        "                torch.save(agt.actor.state_dict(), './checkpoints/checkpoint_actor_{}.pth'.format(i))\n",
        "                torch.save(agt.actor_target.state_dict(), './checkpoints/checkpoint_actor_target_{}.pth'.format(i))\n",
        "                torch.save(agt.critic.state_dict(), './checkpoints/checkpoint_critic_{}.pth'.format(i))\n",
        "                torch.save(agt.critic_target.state_dict(), './checkpoints/checkpoint_critic_target_{}.pth'.format(i))\n",
        "\n",
        "                i += 1\n",
        "\n",
        "            t_start = time.time()\n",
        "            # Keep track of final episode reward\n",
        "            final_ep_rewards.append(np.mean(episode_rewards[-1000:]))\n",
        "            for rew in agent_rewards:\n",
        "                final_ep_ag_rewards.append(np.mean(rew[-1000:]))\n",
        "\n",
        "            \n",
        "        # if len(episode_rewards) > 60000:\n",
        "        #     break\n",
        "\n",
        "        # saves final episode reward for plotting training curve later\n",
        "        if len(episode_rewards) > 100000:\n",
        "            rew_file_name = plots_dir + exp_name + '_rewards.pkl'\n",
        "            os.makedirs(os.path.dirname(rew_file_name), exist_ok=True)\n",
        "            with open(rew_file_name, 'wb') as fp:\n",
        "                pickle.dump(final_ep_rewards, fp)\n",
        "            agrew_file_name = plots_dir + exp_name + '_agrewards.pkl'\n",
        "            os.makedirs(os.path.dirname(agrew_file_name), exist_ok=True)\n",
        "            with open(agrew_file_name, 'wb') as fp:\n",
        "                pickle.dump(final_ep_ag_rewards, fp)\n",
        "            print('...Finished total of {} episodes.'.format(len(episode_rewards)))\n",
        "            break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Ov83CP5DXG",
        "colab_type": "code",
        "outputId": "511cd15a-ae63-4234-8516-8cce388e562c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "class Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "arglist = Namespace(GAMMA=0.95, batch_size=1024, benchmark=False, benchmark_dir='./benchmark_files/', benchmark_iters=100000, display=False, eval=True, exp_name=None, lr=0.01, max_episode_len=25, num_adversaries=0, num_episodes=60000, plots_dir='./learning_curves/', restore=False, saved_episode=50, scenario='simple_tag', tau=0.01)\n",
        "train(arglist= arglist)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DDPG load.\n",
            "MADDPG load.\n",
            "Starting iterations...\n",
            "steps: 24975, episodes: 1000, mean episode reward: -80.13714059557961, time: 30.693\n",
            "steps: 49975, episodes: 2000, mean episode reward: -10.378564943353062, time: 39.177\n",
            "steps: 74975, episodes: 3000, mean episode reward: -1.7050223500112087, time: 39.452\n",
            "steps: 99975, episodes: 4000, mean episode reward: -0.42428945629347403, time: 39.541\n",
            "steps: 124975, episodes: 5000, mean episode reward: -7.829963429471248, time: 39.572\n",
            "steps: 149975, episodes: 6000, mean episode reward: -5.670032880874998, time: 39.763\n",
            "steps: 174975, episodes: 7000, mean episode reward: -3.234661381095169, time: 39.785\n",
            "steps: 199975, episodes: 8000, mean episode reward: -1.0622234886474815, time: 39.499\n",
            "steps: 224975, episodes: 9000, mean episode reward: -0.9360123590890869, time: 39.841\n",
            "steps: 249975, episodes: 10000, mean episode reward: -1.566667839808383, time: 39.656\n",
            "steps: 274975, episodes: 11000, mean episode reward: -1.57789485118357, time: 39.941\n",
            "steps: 299975, episodes: 12000, mean episode reward: -1.510077445472249, time: 39.692\n",
            "steps: 324975, episodes: 13000, mean episode reward: -1.491294310133604, time: 39.702\n",
            "steps: 349975, episodes: 14000, mean episode reward: -1.9916506099395255, time: 40.04\n",
            "steps: 374975, episodes: 15000, mean episode reward: -2.0168239396888166, time: 39.634\n",
            "steps: 399975, episodes: 16000, mean episode reward: -1.479368332953115, time: 40.162\n",
            "steps: 424975, episodes: 17000, mean episode reward: -1.5141470974958393, time: 39.721\n",
            "steps: 449975, episodes: 18000, mean episode reward: -1.2702156022424986, time: 39.69\n",
            "steps: 474975, episodes: 19000, mean episode reward: -1.7106090522750292, time: 39.62\n",
            "steps: 499975, episodes: 20000, mean episode reward: -1.4135923634144514, time: 39.672\n",
            "steps: 524975, episodes: 21000, mean episode reward: -1.482908024757723, time: 39.544\n",
            "steps: 549975, episodes: 22000, mean episode reward: -1.373468539604202, time: 40.449\n",
            "steps: 574975, episodes: 23000, mean episode reward: -1.2436773527603953, time: 39.762\n",
            "steps: 599975, episodes: 24000, mean episode reward: -1.1816731195413255, time: 39.693\n",
            "steps: 624975, episodes: 25000, mean episode reward: -1.240714594645183, time: 39.802\n",
            "steps: 649975, episodes: 26000, mean episode reward: -1.1154068502621126, time: 39.688\n",
            "steps: 674975, episodes: 27000, mean episode reward: -1.0589428961389318, time: 39.669\n",
            "steps: 699975, episodes: 28000, mean episode reward: -1.2893787965352759, time: 39.718\n",
            "steps: 724975, episodes: 29000, mean episode reward: -1.1213635710362078, time: 40.27\n",
            "steps: 749975, episodes: 30000, mean episode reward: -1.0177100943170028, time: 40.151\n",
            "steps: 774975, episodes: 31000, mean episode reward: -1.1729779336249782, time: 39.883\n",
            "steps: 799975, episodes: 32000, mean episode reward: -1.0798105393199084, time: 39.944\n",
            "steps: 824975, episodes: 33000, mean episode reward: -0.8455177803669474, time: 39.801\n",
            "steps: 849975, episodes: 34000, mean episode reward: -0.8800903235186821, time: 39.84\n",
            "steps: 874975, episodes: 35000, mean episode reward: -0.8129536185142807, time: 39.734\n",
            "steps: 899975, episodes: 36000, mean episode reward: -0.8964692140344342, time: 39.689\n",
            "steps: 924975, episodes: 37000, mean episode reward: -0.9993358313110234, time: 39.828\n",
            "steps: 949975, episodes: 38000, mean episode reward: -0.7448283074809255, time: 39.715\n",
            "steps: 974975, episodes: 39000, mean episode reward: -0.9338200860622897, time: 40.161\n",
            "steps: 999975, episodes: 40000, mean episode reward: -1.00491086200652, time: 39.612\n",
            "steps: 1024975, episodes: 41000, mean episode reward: -0.9733322568946965, time: 40.035\n",
            "steps: 1049975, episodes: 42000, mean episode reward: -0.8465784875360921, time: 39.812\n",
            "steps: 1074975, episodes: 43000, mean episode reward: -0.84574486259888, time: 40.031\n",
            "steps: 1099975, episodes: 44000, mean episode reward: -0.9074976086312986, time: 39.631\n",
            "steps: 1124975, episodes: 45000, mean episode reward: -0.9939361434577119, time: 40.155\n",
            "steps: 1149975, episodes: 46000, mean episode reward: -0.9792890745045151, time: 39.775\n",
            "steps: 1174975, episodes: 47000, mean episode reward: -0.9724779286731312, time: 40.16\n",
            "steps: 1199975, episodes: 48000, mean episode reward: -1.3698403687644551, time: 39.89\n",
            "steps: 1224975, episodes: 49000, mean episode reward: -1.1497688157809764, time: 40.121\n",
            "steps: 1249975, episodes: 50000, mean episode reward: -0.9192513939031106, time: 39.756\n",
            "steps: 1274975, episodes: 51000, mean episode reward: -1.0385146069029094, time: 40.043\n",
            "steps: 1299975, episodes: 52000, mean episode reward: -1.0320005661016372, time: 39.666\n",
            "steps: 1324975, episodes: 53000, mean episode reward: -1.360179988306615, time: 40.201\n",
            "steps: 1349975, episodes: 54000, mean episode reward: -1.0004637562559595, time: 40.32\n",
            "steps: 1374975, episodes: 55000, mean episode reward: -1.1607354701502521, time: 40.122\n",
            "steps: 1399975, episodes: 56000, mean episode reward: -0.9046425509379967, time: 39.837\n",
            "steps: 1424975, episodes: 57000, mean episode reward: -1.278675084766896, time: 40.021\n",
            "steps: 1449975, episodes: 58000, mean episode reward: -0.9399380931159573, time: 39.881\n",
            "steps: 1474975, episodes: 59000, mean episode reward: -1.255793191185245, time: 39.791\n",
            "steps: 1499975, episodes: 60000, mean episode reward: -0.9935924843412639, time: 39.808\n",
            "steps: 1524975, episodes: 61000, mean episode reward: -1.0689084759027112, time: 39.906\n",
            "steps: 1549975, episodes: 62000, mean episode reward: -1.0973461583683672, time: 39.691\n",
            "steps: 1574975, episodes: 63000, mean episode reward: -1.1601421290895888, time: 39.794\n",
            "steps: 1599975, episodes: 64000, mean episode reward: -1.1121029609287236, time: 40.032\n",
            "steps: 1624975, episodes: 65000, mean episode reward: -1.0498572855004347, time: 40.135\n",
            "steps: 1649975, episodes: 66000, mean episode reward: -1.1265415660473275, time: 39.861\n",
            "steps: 1674975, episodes: 67000, mean episode reward: -0.925668971172504, time: 40.075\n",
            "steps: 1699975, episodes: 68000, mean episode reward: -0.817383670878989, time: 40.231\n",
            "steps: 1724975, episodes: 69000, mean episode reward: -0.9172844097405566, time: 40.197\n",
            "steps: 1749975, episodes: 70000, mean episode reward: -0.6691693836116772, time: 39.852\n",
            "steps: 1774975, episodes: 71000, mean episode reward: -1.0374248488184468, time: 40.056\n",
            "steps: 1799975, episodes: 72000, mean episode reward: -1.049532875251647, time: 39.815\n",
            "steps: 1824975, episodes: 73000, mean episode reward: -1.107425537643882, time: 39.951\n",
            "steps: 1849975, episodes: 74000, mean episode reward: -0.7239374629712438, time: 39.877\n",
            "steps: 1874975, episodes: 75000, mean episode reward: -1.0416249593728748, time: 39.767\n",
            "steps: 1899975, episodes: 76000, mean episode reward: -1.073839966108417, time: 39.832\n",
            "steps: 1924975, episodes: 77000, mean episode reward: -0.9648371718872908, time: 39.684\n",
            "steps: 1949975, episodes: 78000, mean episode reward: -0.9938000166297029, time: 39.694\n",
            "steps: 1974975, episodes: 79000, mean episode reward: -0.8374610661139463, time: 39.692\n",
            "steps: 1999975, episodes: 80000, mean episode reward: -1.2199872639045661, time: 39.634\n",
            "steps: 2024975, episodes: 81000, mean episode reward: -1.3357648590228168, time: 39.82\n",
            "steps: 2049975, episodes: 82000, mean episode reward: -1.0867073916915586, time: 39.836\n",
            "steps: 2074975, episodes: 83000, mean episode reward: -1.031090567927518, time: 39.887\n",
            "steps: 2099975, episodes: 84000, mean episode reward: -1.3810639142954264, time: 40.013\n",
            "steps: 2124975, episodes: 85000, mean episode reward: -1.1431045869503706, time: 40.003\n",
            "steps: 2149975, episodes: 86000, mean episode reward: -1.13394376556678, time: 39.93\n",
            "steps: 2174975, episodes: 87000, mean episode reward: -1.282924695433256, time: 39.857\n",
            "steps: 2199975, episodes: 88000, mean episode reward: -0.8065872951071937, time: 39.861\n",
            "steps: 2224975, episodes: 89000, mean episode reward: -0.9165172736183036, time: 39.922\n",
            "steps: 2249975, episodes: 90000, mean episode reward: -1.1918684178291428, time: 39.863\n",
            "steps: 2274975, episodes: 91000, mean episode reward: -0.8319533864472453, time: 39.881\n",
            "steps: 2299975, episodes: 92000, mean episode reward: -1.0000450627903439, time: 40.058\n",
            "steps: 2324975, episodes: 93000, mean episode reward: -0.827492915561671, time: 39.906\n",
            "steps: 2349975, episodes: 94000, mean episode reward: -0.9671323715521465, time: 39.986\n",
            "steps: 2374975, episodes: 95000, mean episode reward: -0.8742101234662693, time: 39.911\n",
            "steps: 2399975, episodes: 96000, mean episode reward: -0.8818583898834972, time: 39.839\n",
            "steps: 2424975, episodes: 97000, mean episode reward: -0.9373991391551786, time: 39.954\n",
            "steps: 2449975, episodes: 98000, mean episode reward: -0.7785994227642127, time: 40.493\n",
            "steps: 2474975, episodes: 99000, mean episode reward: -0.8748212068746757, time: 40.064\n",
            "steps: 2499975, episodes: 100000, mean episode reward: -0.9827639658292545, time: 39.964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-656b276758c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0marglist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNamespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenchmark_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./benchmark_files/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbenchmark_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episode_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_adversaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplots_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./learning_curves/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscenario\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'simple_tag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marglist\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0marglist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-58-0e81c134d01a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(arglist)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# saves final episode reward for plotting training curve later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mrew_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplots_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexp_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_rewards.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrew_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrew_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plots_dir' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U10PHIMV5iZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
