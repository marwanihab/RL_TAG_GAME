{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finalTaining.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRv-urycpDKO",
        "colab_type": "text"
      },
      "source": [
        "# This Notebook is to train the model using github repositry on google colab:\n",
        "**The steps of doing so are:**\n",
        "\n",
        "\n",
        "1.   Mount the drive \n",
        "2.   Creating a path to store the github repo files \n",
        "3.   Creating a GIT token to access the files \n",
        "4.   %cd to the path then clonning the repo \n",
        "5.   Importing the files (the method is shown below in the cell)\n",
        "6.   Creating our train function then running it \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u0UvS8jqQ0Q",
        "colab_type": "text"
      },
      "source": [
        "# Mounting the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksGwDuXA3vr4",
        "colab_type": "code",
        "outputId": "effded2e-fc68-433c-8884-6887e718bf17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT) "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP80vddWqXem",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Path and GIT token and making the directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DmkUV7T4DXP",
        "colab_type": "code",
        "outputId": "d1f7fef4-b9f5-4dc5-8c9b-3df4a63c2b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from os.path import join  \n",
        "\n",
        "# path to your project on Google Drive\n",
        "MY_GOOGLE_DRIVE_PATH = 'My Drive/RL_Tag_GameFinal' \n",
        "# replace with your Github username \n",
        "GIT_USERNAME = \"username\" \n",
        "# definitely replace with your\n",
        "GIT_TOKEN = \"{xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx}\"  \n",
        "# Replace with your github repository in this case we want \n",
        "# to clone deep-learning-v2-pytorch repository\n",
        "GIT_REPOSITORY = \"RL_TAG\" \n",
        "\n",
        "PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
        "\n",
        "# It's good to print out the value if you are not sure \n",
        "print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
        "\n",
        "# In case we haven't created the folder already; we will create a folder in the project path \n",
        "!mkdir \"{PROJECT_PATH}\"    \n",
        "\n",
        "#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n",
        "GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "print(\"GIT_PATH: \", GIT_PATH)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PROJECT_PATH:  /content/drive/My Drive/RL_Tag_GameFinal\n",
            "mkdir: cannot create directory ‘/content/drive/My Drive/RL_Tag_GameFinal’: File exists\n",
            "GIT_PATH:  https://{74b3c3dfea907d13d3490e356b8f1246ce3a7b86}@github.com/marwanihab/RL_TAG.git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUtVa5_5qg0r",
        "colab_type": "text"
      },
      "source": [
        "# %cd to the directory and git pull "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSMoJuUf4OKw",
        "colab_type": "code",
        "outputId": "8d55e20e-db36-42a0-aa1d-e05f44b59223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd {PROJECT_PATH}\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/RL_Tag_GameFinal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL8PxM5R4TYD",
        "colab_type": "code",
        "outputId": "6edbca7a-7864-426e-f075-224d2e4b9a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone {GIT_PATH}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'RL_TAG' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTgHz4Sd4W1Y",
        "colab_type": "code",
        "outputId": "9546750a-1b99-4ea5-fd88-cd942111e710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd RL_TAG/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/RL_Tag_GameFinal/RL_TAG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0juQQ1tnk-t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "882ec96a-eef6-447a-d00a-6a3286f5ac28"
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updating 6dccc7e..31992d3\n",
            "Fast-forward\n",
            " .DS_Store                                          | Bin \u001b[31m12292\u001b[m -> \u001b[32m10244\u001b[m bytes\n",
            " .idea/.gitignore                                   |   3 \u001b[32m++\u001b[m\n",
            " .idea/SecondTrial.iml                              |  12 \u001b[32m++++++++\u001b[m\n",
            " .idea/inspectionProfiles/profiles_settings.xml     |   6 \u001b[32m++++\u001b[m\n",
            " .idea/misc.xml                                     |   7 \u001b[32m+++++\u001b[m\n",
            " .idea/modules.xml                                  |   8 \u001b[32m+++++\u001b[m\n",
            " .idea/vcs.xml                                      |   6 \u001b[32m++++\u001b[m\n",
            " checkpoints-20200401T205227Z-001.zip               | Bin \u001b[31m0\u001b[m -> \u001b[32m586076\u001b[m bytes\n",
            " checkpoints-21.zip                                 | Bin \u001b[31m0\u001b[m -> \u001b[32m586424\u001b[m bytes\n",
            " checkpoints/.DS_Store                              | Bin \u001b[31m6148\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_0.pth                 | Bin \u001b[31m79296\u001b[m -> \u001b[32m77184\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_1.pth                 | Bin \u001b[31m79296\u001b[m -> \u001b[32m76128\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_2.pth                 | Bin \u001b[31m79296\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_3.pth                 | Bin \u001b[31m78240\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_target_0.pth          | Bin \u001b[31m79296\u001b[m -> \u001b[32m77184\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_target_1.pth          | Bin \u001b[31m79296\u001b[m -> \u001b[32m76128\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_target_2.pth          | Bin \u001b[31m79296\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_actor_target_3.pth          | Bin \u001b[31m78240\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_0.pth                | Bin \u001b[31m112058\u001b[m -> \u001b[32m77738\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_1.pth                | Bin \u001b[31m112058\u001b[m -> \u001b[32m85658\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_2.pth                | Bin \u001b[31m112058\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_3.pth                | Bin \u001b[31m78794\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_target_0.pth         | Bin \u001b[31m112058\u001b[m -> \u001b[32m77738\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_target_1.pth         | Bin \u001b[31m112058\u001b[m -> \u001b[32m85658\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_target_2.pth         | Bin \u001b[31m112058\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " checkpoints/checkpoint_critic_target_3.pth         | Bin \u001b[31m78794\u001b[m -> \u001b[32m0\u001b[m bytes\n",
            " .../__pycache__/simple_tag.cpython-37.pyc          | Bin \u001b[31m5395\u001b[m -> \u001b[32m4946\u001b[m bytes\n",
            " multiagent/scenarios/simple_tag.py                 |  34 \u001b[32m++++++++++\u001b[m\u001b[31m-----------\u001b[m\n",
            " 28 files changed, 59 insertions(+), 17 deletions(-)\n",
            " create mode 100644 .idea/.gitignore\n",
            " create mode 100644 .idea/SecondTrial.iml\n",
            " create mode 100644 .idea/inspectionProfiles/profiles_settings.xml\n",
            " create mode 100644 .idea/misc.xml\n",
            " create mode 100644 .idea/modules.xml\n",
            " create mode 100644 .idea/vcs.xml\n",
            " create mode 100644 checkpoints-20200401T205227Z-001.zip\n",
            " create mode 100644 checkpoints-21.zip\n",
            " delete mode 100644 checkpoints/.DS_Store\n",
            " delete mode 100644 checkpoints/checkpoint_actor_2.pth\n",
            " delete mode 100644 checkpoints/checkpoint_actor_3.pth\n",
            " delete mode 100644 checkpoints/checkpoint_actor_target_2.pth\n",
            " delete mode 100644 checkpoints/checkpoint_actor_target_3.pth\n",
            " delete mode 100644 checkpoints/checkpoint_critic_2.pth\n",
            " delete mode 100644 checkpoints/checkpoint_critic_3.pth\n",
            " delete mode 100644 checkpoints/checkpoint_critic_target_2.pth\n",
            " delete mode 100644 checkpoints/checkpoint_critic_target_3.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC7e50I54a0G",
        "colab_type": "code",
        "outputId": "aaec019d-4f95-4be4-f3cc-e36e2d34a4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "!pip uninstall gym\n",
        "!pip install gym==0.10.5"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling gym-0.17.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/gym-0.17.1.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/gym/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled gym-0.17.1\n",
            "Collecting gym==0.10.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/50/ed4a03d2be47ffd043be2ee514f329ce45d98a30fe2d1b9c61dea5a9d861/gym-0.10.5.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (1.18.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym==0.10.5) (1.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym==0.10.5) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym==0.10.5) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.10.5-cp36-none-any.whl size=1581309 sha256=640ac5ab74ae8e0b1e7d607f9e80a84ca6b37db7ab0b6641645b5e9b93475712\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/14/71/f4ab006b1e6ff75c2b54985c2f98d0644fffe9c1dddc670925\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "Successfully installed gym-0.10.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSzc6peaqs3a",
        "colab_type": "text"
      },
      "source": [
        "# Most important step creating a modules for the necessary \".py\" file that we need to import to run the train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RklAbpwF4h88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imp \n",
        "Agent = imp.new_module('agent')\n",
        "exec(open(\"./agent.py\").read(), Agent.__dict__)\n",
        "\n",
        "actor_critic_model = imp.new_module('actor_critic_model')\n",
        "exec(open(\"./actor_critic_model.py\").read(), actor_critic_model.__dict__)\n",
        "\n",
        "ornsteinUhlenbeck = imp.new_module('ornsteinUhlenbeck')\n",
        "exec(open(\"./ornsteinUhlenbeck.py\").read(), ornsteinUhlenbeck.__dict__)\n",
        "\n",
        "replay_buffer = imp.new_module('replay_buffer')\n",
        "exec(open(\"./replay_buffer.py\").read(), replay_buffer.__dict__)\n",
        "\n",
        "core = imp.new_module('core.py')\n",
        "exec(open(\"./multiagent/core.py\").read(), core.__dict__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WLIu-FW4nU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "environment = imp.new_module('environment.py')\n",
        "exec(open(\"./multiagent/environment.py\").read(), environment.__dict__)\n",
        "\n",
        "multi_discrete = imp.new_module('multi_discrete.py')\n",
        "exec(open(\"./multiagent/multi_discrete.py\").read(), multi_discrete.__dict__)\n",
        "\n",
        "# policy = imp.new_module('policy.py')\n",
        "# exec(open(\"./multiagent/policy.py\").read(), policy.__dict__)\n",
        "\n",
        "# rendering = imp.new_module('rendering.py')\n",
        "# exec(open(\"./multiagent/rendering.py\").read(), rendering.__dict__)\n",
        "\n",
        "scenario = imp.new_module('scenario.py')\n",
        "exec(open(\"./multiagent/scenario.py\").read(), scenario.__dict__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqzyHn3s4qjH",
        "colab_type": "code",
        "outputId": "93e7ca45-4fef-4be8-9ea0-1f08d07e1e8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "!pip show torchvision"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: torchvision\n",
            "Version: 0.5.0\n",
            "Summary: image and video datasets and models for torch deep learning\n",
            "Home-page: https://github.com/pytorch/vision\n",
            "Author: PyTorch Core Team\n",
            "Author-email: soumith@pytorch.org\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: pillow, torch, numpy, six\n",
            "Required-by: fastai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hsTxEwo40GC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "from agent import Agent\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import pickle\n",
        "from gym.spaces import Box\n",
        "from actor_critic_model import Actor, Critic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RdQIFRP47cF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_env(scenario_name, arglist):\n",
        "    from multiagent.environment import MultiAgentEnv\n",
        "    import multiagent.scenarios as scenarios\n",
        "\n",
        "    # load scenario from script\n",
        "    scenario = scenarios.load(scenario_name + \".py\").Scenario()\n",
        "    # create world\n",
        "    world = scenario.make_world()\n",
        "    # create multiagent environment\n",
        "    if arglist.benchmark:\n",
        "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation, scenario.benchmark_data)\n",
        "    else:\n",
        "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)\n",
        "    return env\n",
        "\n",
        "\n",
        "def _algo_mode_from_agents(env):\n",
        "    algo_mode = []\n",
        "\n",
        "    for agent in env.agents:\n",
        "        if agent.adversary:  # adversary\n",
        "            algo_mode.append('DDPG')  # MADDPG\n",
        "        else:\n",
        "            algo_mode.append('MADDPG')\n",
        "    return algo_mode\n",
        "\n",
        "\n",
        "def create_agents(env, arglist):\n",
        "    agents = []\n",
        "    algo_mode = _algo_mode_from_agents(env=env)\n",
        "\n",
        "    obs_shapes = [env.observation_space[i].shape for i in range(env.n)]\n",
        "    actions_shape_n = [env.action_space[i].n for i in range(env.n)]\n",
        "    actions_n = 0\n",
        "    obs_shape_n = 0\n",
        "\n",
        "    for actions in actions_shape_n:\n",
        "        actions_n += actions\n",
        "    for obs_shape in obs_shapes:\n",
        "        obs_shape_n += obs_shape[0]\n",
        "\n",
        "    for i, action_space, observation_space, algo in zip(range(len(env.action_space)), env.action_space,\n",
        "                                                        env.observation_space, algo_mode):\n",
        "\n",
        "        if isinstance(action_space, Box):\n",
        "            discrete_action = False\n",
        "        else:\n",
        "            discrete_action = True\n",
        "\n",
        "        if algo == 'MADDPG':\n",
        "            print('MADDPG load.')\n",
        "            critic = Critic(obs_shape_n, actions_n).to(device)\n",
        "            actor = Actor(observation_space.shape[0], action_space.n).to(device)\n",
        "            target_critic = Critic(obs_shape_n, actions_n, arglist.tau).to(device)\n",
        "            target_actor = Actor(observation_space.shape[0], action_space.n, arglist.tau).to(device)\n",
        "        else:\n",
        "            print('DDPG load.')\n",
        "            critic = Critic(observation_space.shape[0], action_space.n).to(device)\n",
        "            actor = Actor(observation_space.shape[0], action_space.n).to(device)\n",
        "            target_critic = Critic(observation_space.shape[0], action_space.n, arglist.tau).to(device)\n",
        "            target_actor = Actor(observation_space.shape[0], action_space.n, arglist.tau).to(device)\n",
        "        actor.eval()\n",
        "        critic.eval()\n",
        "        target_actor.eval()\n",
        "        target_critic.eval()\n",
        "        agents.append(\n",
        "            Agent(i, actor, critic, target_actor, target_critic, arglist.eval, discrete_action, arglist, algo))\n",
        "    return agents\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMZE3AoK5Adg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(arglist):\n",
        "    env = make_env(scenario_name=\"simple_tag\", arglist=arglist)\n",
        "    # ACTORS = 1\n",
        "    # env = EnvWrapper(arglist.scenario, ACTORS, arglist.saved_episode)\n",
        "    agents = create_agents(env, arglist)\n",
        "    max_episode_len = 25\n",
        "\n",
        "    if arglist.display:\n",
        "        for i in range(len(agents)):\n",
        "            actor = agents[i].actor\n",
        "            actor_ckpt = torch.load('./checkpoints/checkpoint_actor_{}.pth'.format(i), map_location='cpu')\n",
        "            actor.load_state_dict(actor_ckpt)\n",
        "            actor_target = agents[i].actor_target\n",
        "            actor_target_ckpt = torch.load('./checkpoints/checkpoint_actor_target_{}.pth'.format(i), map_location='cpu')\n",
        "            actor_target.load_state_dict(actor_target_ckpt)\n",
        "            critic = agents[i].critic\n",
        "            critic_ckpt = torch.load('./checkpoints/checkpoint_critic_{}.pth'.format(i), map_location='cpu')\n",
        "            critic.load_state_dict(critic_ckpt)\n",
        "            critic_target = agents[i].critic_target\n",
        "            critic_target_ckpt = torch.load('./checkpoints/checkpoint_critic_target_{}.pth'.format(i),\n",
        "                                            map_location='cpu')\n",
        "            critic_target.load_state_dict(critic_target_ckpt)\n",
        "\n",
        "    final_ep_rewards = []\n",
        "    final_ep_ag_rewards = []\n",
        "    episode_rewards = [0.0]\n",
        "    agent_rewards = [[0.0] for _ in range(env.n)]\n",
        "    agent_info = [[[]]]\n",
        "    obs_n = env.reset()\n",
        "    episode_step = 0\n",
        "    train_step = 0\n",
        "    t_start = time.time()\n",
        "\n",
        "    print('Starting iterations...')\n",
        "    while True:\n",
        "        # get action\n",
        "        for agent in agents:\n",
        "            agent.reset()\n",
        "\n",
        "        # action_n = []\n",
        "        # for obs in obs_n:\n",
        "        #     actions = []\n",
        "        #     for i, agent in enumerate(agents):\n",
        "        #         action = agent.act(obs[i], add_noise=False)\n",
        "        #         actions.append(action)\n",
        "        #     action_n.append(actions)\n",
        "\n",
        "        action_n = [agent.act(obs, add_noise=False) for agent, obs in zip(agents, obs_n)]\n",
        "\n",
        "        # environment step\n",
        "        new_obs_n, rew_n, done_n, info_n = env.step(action_n)\n",
        "        episode_step += 1\n",
        "        done = all(done_n)\n",
        "        terminal = (episode_step >= max_episode_len)\n",
        "        # collect experience\n",
        "        for i, agent in enumerate(agents):\n",
        "            agent.experience(obs_n[i], action_n[i], rew_n[i], new_obs_n[i], done_n[i])\n",
        "        obs_n = new_obs_n\n",
        "        # print(rew_n)\n",
        "        for i, rew in enumerate(rew_n):\n",
        "            episode_rewards[-1] += rew\n",
        "            agent_rewards[i][-1] += rew\n",
        "\n",
        "        if done or terminal:\n",
        "            obs_n = env.reset()\n",
        "            episode_step = 0\n",
        "            episode_rewards.append(0)\n",
        "            # print(episode_rewards)\n",
        "            for a in agent_rewards:\n",
        "                a.append(0)\n",
        "            agent_info.append([[]])\n",
        "\n",
        "        # increment global step counter\n",
        "        train_step += 1\n",
        "        # for benchmarking learned policies\n",
        "        if arglist.benchmark:\n",
        "            for i, info in enumerate(info_n):\n",
        "                agent_info[-1][i].append(info_n['n'])\n",
        "            if train_step > arglist.benchmark_iters and (done or terminal) and (len(episode_rewards) % 1000 == 0):\n",
        "                file_name = arglist.benchmark_dir + arglist.exp_name + '.pkl'\n",
        "                print('Finished benchmarking, now saving...')\n",
        "                with open(file_name, 'wb') as fp:\n",
        "                    pickle.dump(agent_info[:-1], fp)\n",
        "                # break\n",
        "            # continue\n",
        "\n",
        "        # for displaying learned policies\n",
        "        if arglist.display:\n",
        "            time.sleep(0.1)\n",
        "            env.render()\n",
        "            continue\n",
        "\n",
        "        # update all trainers, if not in display or benchmark mode\n",
        "        loss = None\n",
        "        for agent in agents:\n",
        "            agent.preupdate()\n",
        "        for agent in agents:\n",
        "            loss = agent.step(agents, train_step, terminal)\n",
        "\n",
        "        # save model, display training output\n",
        "        if terminal and (len(episode_rewards) % 1000 == 0):  # 25 and 1000\n",
        "\n",
        "            print(\"steps: {}, episodes: {}, mean episode reward: {}, time: {}\".format(\n",
        "                train_step, len(episode_rewards), np.mean(episode_rewards[-1000:]), round(time.time() - t_start, 3)))\n",
        "            # plotter.plot('Episode Rewards', 'Rewards', 'Training', len(episode_rewards),\n",
        "            #              np.mean(episode_rewards[-1000:]))\n",
        "            i = 0\n",
        "            for agt in agents:\n",
        "                torch.save(agt.actor.state_dict(), './checkpoints/checkpoint_actor_{}.pth'.format(i))\n",
        "                torch.save(agt.actor_target.state_dict(), './checkpoints/checkpoint_actor_target_{}.pth'.format(i))\n",
        "                torch.save(agt.critic.state_dict(), './checkpoints/checkpoint_critic_{}.pth'.format(i))\n",
        "                torch.save(agt.critic_target.state_dict(), './checkpoints/checkpoint_critic_target_{}.pth'.format(i))\n",
        "\n",
        "                i += 1\n",
        "\n",
        "            t_start = time.time()\n",
        "            # Keep track of final episode reward\n",
        "            final_ep_rewards.append(np.mean(episode_rewards[-1000:]))\n",
        "            for rew in agent_rewards:\n",
        "                final_ep_ag_rewards.append(np.mean(rew[-1000:]))\n",
        "\n",
        "            \n",
        "        # if len(episode_rewards) > 60000:\n",
        "        #     break\n",
        "\n",
        "        # saves final episode reward for plotting training curve later\n",
        "        if len(episode_rewards) > 100000:\n",
        "            rew_file_name = plots_dir + exp_name + '_rewards.pkl'\n",
        "            os.makedirs(os.path.dirname(rew_file_name), exist_ok=True)\n",
        "            with open(rew_file_name, 'wb') as fp:\n",
        "                pickle.dump(final_ep_rewards, fp)\n",
        "            agrew_file_name = plots_dir + exp_name + '_agrewards.pkl'\n",
        "            os.makedirs(os.path.dirname(agrew_file_name), exist_ok=True)\n",
        "            with open(agrew_file_name, 'wb') as fp:\n",
        "                pickle.dump(final_ep_ag_rewards, fp)\n",
        "            print('...Finished total of {} episodes.'.format(len(episode_rewards)))\n",
        "            break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Ov83CP5DXG",
        "colab_type": "code",
        "outputId": "511cd15a-ae63-4234-8516-8cce388e562c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "class Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "\n",
        "arglist = Namespace(GAMMA=0.95, batch_size=1024, benchmark=False, benchmark_dir='./benchmark_files/', benchmark_iters=100000, display=False, eval=True, exp_name=None, lr=0.01, max_episode_len=25, num_adversaries=0, num_episodes=60000, plots_dir='./learning_curves/', restore=False, saved_episode=50, scenario='simple_tag', tau=0.01)\n",
        "train(arglist= arglist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U10PHIMV5iZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
